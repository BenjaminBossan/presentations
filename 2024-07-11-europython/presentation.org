#+title: Fine-tuning large models on local hardware
#+Author: Benjamin Bossan
#+Date: 2024-07-11
#+OPTIONS: toc:nil
#+REVEAL_TITLE_SLIDE: %t
#+MACRO: color @@html:<font color="$1">$2</font>@@
#+REVEAL_THEME: black
#+REVEAL_EXTRA_CSS: ./local.css
#+REVEAL_SLIDE_FOOTER:
#+OPTIONS: reveal_single_file:t
#+OPTIONS: num:nil

* Fine-tuning large models on local hardware
slides are available at:

https://github.com/BenjaminBossan/presentations/
#+attr_html: :width 250px :align center
[[./assets/qr.png]]
* The problem
** Training a Llama model locally
What's wrong with this code?
#+begin_src python
import torch
from transformers import AutoModelForCausalLM
# ...
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda")
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
for batch in data_loader:
    optimizer.zero_grad()
    outputs = model(**batch)
    outputs.loss.backward()
    optimizer.step()
#+end_src
** Out of memory ðŸ˜±
:PROPERTIES:
:reveal_background: linear-gradient(to left, #910830, #521623)
:END:
#+begin_src bash
  File "code/train.py", line 253, in <module>
    train(args.model_id, args.rank, args.dtype, args.monitor_tensors, args.max_seq_length, args.batch_size, args.max_steps)
  File "code/train.py", line 121, in train
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device_map)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "env/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "env/site-packages/transformers/modeling_utils.py", line 3754, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "env/site-packages/transformers/modeling_utils.py", line 4214, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "env/site-packages/transformers/modeling_utils.py", line 887, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "env/site-packages/accelerate/utils/modeling.py", line 400, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU
#+end_src
** LLMs are very hungry for memory
~Llama3 8B~ loaded in float16/bfloat16:
| module           | # params      | size  |
|------------------+---------------+-------|
| Embedding        | 525,336,576   | 1GB   |
| Linear (weights) | 6,979,321,856 | 13 GB |
| LlamaRMSNorm     | 266,240       | 500KB |
In total, loading the model requires **14GB**
** LLMs are very hungry for memory
Training requires even more GPU memory:
#+attr_reveal: :frag (frag-style)
- Add gradients: same size as parameters
- Add optimizer states: 2x the size of parameters
- In total, we need enough memory for 4Ã— the size of the model
  - This means we need **at least 56GB** to train Llama3 8B.
* Parameter-efficient fine-tuning methods
** ðŸ¤— PEFT
#+attr_reveal: :frag (frag-style)
- Multitude of methods to decrease memory required for /training/
- Achieved by reducing the number of /trainable/ parameters
- Many convience functions (more later)
#+attr_reveal: :frag (frag-style)
However:
#+attr_reveal: :frag (frag-style)
- Training is not necessarily faster (but often is in practice)
- No memory benefit for inference
** LoRA: Low rank adapters
LoRA is the most popular parameter-efficient fine-tuning method

Paper: https://arxiv.org/abs/2106.09685
** LoRA: Low rank adapters
Decompose updates of weight ~W~ into two low rank matrices ~A~ and ~B~, e.g.:

~W~ is 1024Ã—1024

~A~ is 8Ã—1024

~B~ is 1024Ã—8

\begin{equation}
h_{linear} = W \cdot X + b
\\
h_{lora} = (W + \Delta W) \cdot X + b
\\
\Delta W = B \cdot A
\end{equation}
** LoRA illustrated
#+CAPTION: LoRA illustrated
[[./assets/lora.png]]
** LoRA code
Simplified LoRA code for linear layers:
#+begin_src python
class LoraLinear(nn.Linaer):
    def __init__(self, base_layer: nn.Linear, rank: int):
        self.base_layer = base_layer
        self.lora_A = nn.Linear(base_layer.in_features, rank, bias=False)
        self.lora_B = nn.Linear(rank, base_layer.out_features, bias=False)

    def forward(self, x):
        return self.base_layer(x) + self.lora_B(self.lora_A(x))
#+end_src
** Wait a second: LoRA adds more parameters, how does that reduce the required memory?
** Calculation of memory requirements
#+attr_reveal: :frag (frag-style)
- Remember: 3/4 of memory is reserved for gradients and optimizer states
- Only required for /trainable/ parameters
- LoRA decreases number of trainable parameters
- â†’ less memory despite having more parameters in total
- Bonus: Only save the LoRA weights: very small file size
** PEFT code
#+begin_src python
from transformers import AutoModelForCausalLM
from peft import LoraConfig, get_peft_model

base_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device_map)
config = LoraConfig(r=32)
model = get_peft_model(base_model, config)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
for batch in data_loader:
    ...
#+end_src
** Memory requirements for popular models
| Model                        | Full fine-tuning (float16) | LoRA fine-tuning (rank 32) |
|------------------------------+----------------------------+----------------------------|
| meta-llama/Meta-Llama-3-8B   | 55.92 GB                   | /14.92 GB/                 |
| meta-llama/Meta-Llama-3-70B  | 517.84 GB                  | 134.09 GB                  |
| mistralai/Mistral-7B-v0.3    | 53.0 GB                    | /14.19 GB/                 |
| Qwen/Qwen2-1.5B              | 11.5 GB                    | **3.29 GB**                |
| Qwen/Qwen2-72B               | 532.42 GB                  | 137.81 GB                  |
| google/gemma-2-9b            | 68.86 GB                   | /18.42 GB/                 |
| google/gemma-2-27b           | 202.86 GB                  | 53.27 GB                   |
* PEFT and quantization
** Quantization of neural nets
#+attr_reveal: :frag (frag-style)
- Float32 requires 8 bytes, float16 4 bytes
- Quantization: Load weights in lower precision, e.g. int8 or int4 â†’ 1 and 0.5 bytes, respectively
- Going from float16 to int4: 4Ã— memory reduction
- However, lower precision results in degraded quality
** Quantization code
#+begin_src python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(load_in_4bit=True, ...)
model = AutoModelForCausalLM.from_pretrained(
    model_id, device_map=device_map, quantization_config=bnb_config
)
#+end_src
** Training quantized models is not possible
- Quantized weights are integers
- Gradients for these weights cannot be calculated
- No gradients means no training
** PEFT to the rescue
- With LoRA and similar methods, we don't need gradients on the base model weights
- LoRA weights are still loaded in float32 â†’ gradients â†’ training
- Quantization + LoRA = QLoRA: training with extra small memory footprints
Paper: https://arxiv.org/abs/2305.14314
** Code for quantized LoRA (QLoRA)
Using [[https://github.com/TimDettmers/bitsandbytes][bitsandbytes]]
#+begin_src python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

bnb_config = BitsAndBytesConfig(load_in_4bit=True, ...)
base_model = AutoModelForCausalLM.from_pretrained(
    model_id, device_map=device_map, quantization_config=bnb_config
)
config = LoraConfig(r=32)
model = get_peft_model(base_model, config)
for batch in data_loader:
    ...
#+end_src
[[https://huggingface.co/blog/4bit-transformers-bitsandbytes][blog post]]
** Memory requirements for popular models
| Model                       | Full fine-tuning (int4) | LoRA fine-tuning (rank 32) |
|-----------------------------+-------------------------+----------------------------|
| meta-llama/Meta-Llama-3-8B  | 16.92 GB                | **5.17 GB**                |
| meta-llama/Meta-Llama-3-70B | 135.34 GB               | 38.46 GB                   |
| mistralai/Mistral-7B-v0.3   | 14.0 GB                 | **4.44 GB**                |
| Qwen/Qwen2-1.5B             | 4.18 GB                 | **1.46 GB**                |
| Qwen/Qwen2-72B              | 140.08 GB               | 39.73 GB                   |
| google/gemma-2-9b           | 22.34 GB                | **6.79 GB**                |
| google/gemma-2-27b          | 57.31 GB                | /16.88 GB/                 |
* PEFT features
** Flexibility when it comes to models and adapter methods
#+attr_reveal: :frag (frag-style)
- Adapter methods other than LoRA:
  #+attr_reveal: :frag (frag-style)
  * Specific for language models: Llama-Adapter, Multitask Prompt Tuning, P-tuning, Prefix Tuning, Prompt Tuning
  * Especially parameter efficient: AdaLoRA, IAÂ³, LayerNorm Tuning, VeRA
  * Good for diffusion models: LoHa, LoKr, OFT, BOFT
  * Routing: Polytropon
#+attr_reveal: :frag (frag-style)
- Granular choice of what layers to target
- Possibility for full fine-tuning of select layers if needed (~modules_to_save~)
** Flexibility when it comes to models and adapter methods
#+begin_src python
from peft import LoraConfig, PeftModel, get_peft_model

base_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device_map)
config = LoraConfig(rank=16, target_modules=[...], modules_to_save=[...])
model = get_peft_model(base_model, config)
# train
...
model.save_pretrained(<path>)
# later
base_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device_map)
loaded = PeftModel.from_pretrained(base_model, path)
#+end_src
** LoRA options
- Layer types: ~Linear, Conv2d, Embedding~
- [[https://huggingface.co/docs/peft/main/en/developer_guides/quantization][Quantization]]: bitsandbytes, GPTQ, AQLM, AWQ, EETQ, HQQ
- [[https://huggingface.co/docs/peft/main/en/developer_guides/lora#initialization][Initialzation]]: RS LoRA, LoftQ, PiSSA, OLoRA
- [[https://huggingface.co/docs/peft/main/en/developer_guides/lora#weight-decomposed-low-rank-adaptation-dora][DoRA]]: Weight-Decomposed Low-Rank Adaptation
- [[https://huggingface.co/docs/peft/main/en/developer_guides/lora#memory-efficient-layer-replication-with-lora][Layer replication]]
- Distributed training: DDP, [[https://huggingface.co/docs/peft/main/en/accelerate/deepspeed][DeepSpeed]], [[https://huggingface.co/docs/peft/main/en/accelerate/fsdp][FSDP]]
** Advanced LoRA features in PEFT
- [[https://huggingface.co/docs/peft/v0.11.0/en/package_reference/peft_model#peft.PeftModel.load_adapter][Loading]] multiple adapters and [[https://huggingface.co/docs/peft/v0.11.0/en/package_reference/peft_model#peft.PeftModel.set_adapter][switching]]
- Batches containing [[https://huggingface.co/docs/peft/main/en/developer_guides/lora#inference-with-different-lora-adapters-in-the-same-batch][mixed adapters]] for inference
- Temporarily [[https://huggingface.co/docs/peft/v0.11.0/en/package_reference/peft_model#peft.PeftModel.disable_adapter][disabling]] adapters
- [[https://huggingface.co/docs/peft/v0.11.0/en/package_reference/lora#peft.LoraModel.merge_and_unload][Merging]] of LoRA into the base model
- [[https://huggingface.co/docs/peft/main/en/developer_guides/model_merging#merge-method][Merging of different LoRA adapaters]]
- [[https://huggingface.co/docs/peft/main/en/developer_guides/torch_compile][partial support]] for ~torch.compile~
** Tips for getting started
#+attr_reveal: :frag (frag-style)
- Start with small model, increase size only when necessary
- Check if better prompting is not enough (LLM)
- Try LoRA first: most help online, feature rich
- Do a quick end-to-end run before full training
- Start by targeting all linear layers (~target_modules="all-linear"~)
- Increase rank if underfitting, decrease if overfitting
- Try higher learning rate, greater batch size
- Try different [[https://huggingface.co/docs/peft/developer_guides/lora#initialization][initialization schemes]] (esp. when quantizing)
* Broader ecosystem
** Hugging Face integrations
- [[https://huggingface.co/docs/hub/index][ðŸ¤— Hugging Face Hub]]: Load from, and save to, the HF Hub
- [[https://github.com/huggingface/diffusers/][ðŸ¤— diffusers]]: Diffusion models with LoRA
- [[https://huggingface.co/docs/trl/index][ðŸ¤— TRL]]: Easy reinforcement learning with PEFT model
- [[https://huggingface.co/docs/transformers/index][ðŸ¤— transformers]]: Load LoRA directly into transformers models:
#+begin_src python
model = AutoModel.from_pretrained(...)
# add new adapter
model.add_adapter(lora_config, adapter_name)
# load trained adapter
model.load_adapter(path_to_lora, adapter_name)
# switch adapters
model.set_adapter(adapter_name)
#+end_src
** Others
More specialized tools that are optimized for specific models/use cases
- [[https://github.com/unslothai/unsloth][Unsloth]]: Optimization for speed and even less memory
- [[https://github.com/OpenAccess-AI-Collective/axolotl][axolotl]]: Streamlined fine-tuning incl. PEFT
- [[https://github.com/pytorch/torchtune][torchtune]]: Pure PyTorch fine-tuning, optional conversion to PEFT
- Serving hundreds of LoRA adapters efficiently: [[https://github.com/predibase/lorax][LoRAX]], [[https://github.com/punica-ai/punica][punica]], or [[https://github.com/S-LoRA/S-LoRA][S-LoRA]]
* More resources
- [[https://huggingface.co/docs/peft/index][PEFT docs]]
- [[https://github.com/huggingface/peft][PEFT GitHub repo]]
- Slides: https://github.com/BenjaminBossan/presentations/
#+attr_html: :width 250px :align center
[[./assets/qr.png]]
