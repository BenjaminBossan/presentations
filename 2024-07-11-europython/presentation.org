#+title: Fine-tuning large models on local hardware
#+Author: Benjamin Bossan
#+Date: 2024-07-11
#+OPTIONS: toc:nil
#+REVEAL_TITLE_SLIDE: %t
#+MACRO: color @@html:<font color="$1">$2</font>@@
#+REVEAL_THEME: black
#+REVEAL_EXTRA_CSS: ./local.css
#+REVEAL_SLIDE_FOOTER:
#+OPTIONS: reveal_single_file:t
#+OPTIONS: num:nil

* The problem
** Training a Llama model locally
/show correct looking training code/
What's wrong with this code?
** Out of memory
/scary image of OOM error/
Even if the model perfectly fits into memory, training is not possible
** Why so much memory is required?
Llama3 8B loaded in float16/bfloat16
- Embedding: 525336576 => 1GB
- Linear weights: 6979321856 => 13GB
- LlamaRMSNorm: 266240 => 500KB
total of **14GB** to load the model
** Why so much memory is required?
Training requires even more memory:
- Gradients: same size as parameters
- Adam optimizer: 2x the size of parameters
- Activations: hard to tell, depends on data, model, etc.
at least **56GB** required for full fine-tuning
* Parameter-efficient fine-tuning methods
** The general idea behind PEFT
** LoRA: Low rank adapters
- LoRA is the most popular parameter-efficient fine-tuning method
- Decompose updates of weight ~W~ into two low rank matrices ~A~ and ~B~
- ~f(X) = W@X + b => f(X) = (W + W')@X + b~
- ~W' = A@B~, where ~A~ and ~B~ are low rank
https://arxiv.org/abs/2106.09685
** LoRA illustrated
#+CAPTION: LoRA illustrated
[[./assets/lora.png]]
** How come that more parameters require less memory?
* Results
* Broader overview
** PEFT methods that are implemented
** LoRA options
** Quantization
** (Distributed) Training
** Advanced features
- loading multiple adapters
- disabling adapters
- merging into the base model
- merging of LoRA adapaters
- mixed LoRA adapter inference
** Broader ecosystem
