#+title: Fine-tuning large models on local hardware
#+Author: Benjamin Bossan
#+Date: 2024-07-11
#+OPTIONS: toc:nil
#+REVEAL_TITLE_SLIDE: %t
#+MACRO: color @@html:<font color="$1">$2</font>@@
#+REVEAL_THEME: black
#+REVEAL_EXTRA_CSS: ./local.css
#+REVEAL_SLIDE_FOOTER:
#+OPTIONS: reveal_single_file:t
#+OPTIONS: num:nil

* The problem
** Training a Llama model locally
/show correct looking training code/
What's wrong with this code?
** Out of memory
/scary image of OOM error/
Even if the model perfectly fits into memory, training is not possible
** Why do LLMs require so much memory?
~Llama3 8B~ loaded in float16/bfloat16:
| module           | # params      | size  |
|------------------+---------------+-------|
| Embedding        | 525,336,576   | 1GB   |
| Linear (weights) | 6,979,321,856 | 13 GB |
| LlamaRMSNorm     | 266,240       | 500KB |
In total, loading the model requires **14GB**
** Why do LLMs require so much memory?
Training requires even more memory:
- Gradients: same size as parameters
- Adam optimizer states: 2x the size of parameters
This means we need **at least 56GB** to train Llama3 8B.
On top of this, we need to consider the sizes of the activations, but those depends on data, model, etc.
* Parameter-efficient fine-tuning methods
** The general idea behind PEFT
** LoRA: Low rank adapters
- LoRA is the most popular parameter-efficient fine-tuning method
- Decompose updates of weight ~W~ into two low rank matrices ~A~ and ~B~ (e.g. ~W~ is 1024x1024, ~A~ is 1024x8 and ~B~ is 8x1024)
\begin{equation}
f(X) = W \cdot X + b
\\
f(X) = (W + \Delta W) \cdot X + b
\end{equation}
where
\begin{equation}
\Delta W = A \cdot B
\end{equation}
https://arxiv.org/abs/2106.09685
** LoRA illustrated
#+CAPTION: LoRA illustrated
[[./assets/lora.png]]
** How come that more parameters require less memory?
* Results
* Broader overview
** PEFT methods that are implemented
** LoRA options
** Quantization
** (Distributed) Training
** Advanced features
- loading multiple adapters
- disabling adapters
- merging into the base model
- merging of LoRA adapaters
- mixed LoRA adapter inference
** Broader ecosystem
